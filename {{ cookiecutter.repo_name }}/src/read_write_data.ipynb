{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "try:\n",
    "    import pandas_redshift as pr\n",
    "except:\n",
    "    !pip install pandas_redshift\n",
    "    import pandas_redshift as pr \n",
    "    \n",
    "import boto3\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import timedelta\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo ip route del default\n",
    "!sudo ip route add default via 10.84.0.165 dev eth2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadData:\n",
    "    def __init__(self, credentials_file = './config/redshift.json'):\n",
    "        with open(credentials_file) as raw_file:\n",
    "            self.__creds = json.loads(raw_file.read())\n",
    "\n",
    "    def _read_csv_file(self, file):\n",
    "        return pd.read_csv(file, low_memory=False)\n",
    "        try:\n",
    "            return pd.read_csv(file, low_memory=False, date_parser = pd.to_datetime, parse_dates=[\"created_at\"])\n",
    "        except:\n",
    "            return pd.read_csv(file, low_memory=False)\n",
    "\n",
    "    def __read_path(self, all_files):  \n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            result = executor.map(self._read_csv_file, all_files)\n",
    "        df_temp = pd.concat(result)\n",
    "        return df_temp\n",
    "        \n",
    "    def read_in_parallel_from_path(self, file_path):\n",
    "        \"\"\"\n",
    "        read data from folder path in parallel\n",
    "        \"\"\"\n",
    "        print('reading...')\n",
    "        start_time = time.time()\n",
    "        all_files = glob.glob(file_path, recursive=True)\n",
    "        \n",
    "        df_temp = self.__read_path(all_files)\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        print('done with time:', str(timedelta(seconds=time_elapsed)))\n",
    "        return df_temp\n",
    "\n",
    "    def read_in_parallel_from_s3(self, bucket, file_path):\n",
    "        \"\"\"\n",
    "        read data from folder in s3 bucket in parallel\n",
    "        \"\"\"\n",
    "        print('reading...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        all_objects = s3.list_objects(Bucket = bucket, Prefix=file_path)\n",
    "        files = [f\"s3://{bucket}/{i['Key']}\" for i in all_objects['Contents']]\n",
    "        \n",
    "        df_temp = self.__read_path(all_files)\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        print('done with time:', str(timedelta(seconds=time_elapsed)))\n",
    "        return df_temp\n",
    "             \n",
    "             \n",
    "    def __verify_arguments(self, country, signup_app_id, device):\n",
    "        \"\"\"\n",
    "        verify the type of the arguments and prepare for quary\n",
    "        \"\"\"\n",
    "        if type(country) != list:\n",
    "                country = list([country])\n",
    "\n",
    "        if type(signup_app_id) != list:\n",
    "            signup_app_id = list([signup_app_id])\n",
    "\n",
    "        if type(device) != list:\n",
    "            device =  list([device])\n",
    "\n",
    "        country = ','.join([f\"'{i}'\" for i in country])\n",
    "        signup_app_id = ','.join([f\"'{i}'\" for i in signup_app_id])\n",
    "        device = ','.join([f\"'{i}'\" for i in device])\n",
    "        return country, signup_app_id, device\n",
    "    \n",
    "    def read_redshift_users(self, users_list):\n",
    "        \"\"\"\n",
    "        read data from redshift based on specified users_list\n",
    "        users_list: list of users, or one user\n",
    "        signup_app_id: list or one signup id\n",
    "        country :  list or one country\n",
    "        device  :  list or one device\n",
    "        \"\"\"\n",
    "        print('reading...')\n",
    "        \n",
    "        if type(users_list) != list:\n",
    "                users_list =  list([users_list])\n",
    "\n",
    "        users_list = ', '.join([str(i) for i in users_list])\n",
    "            \n",
    "        query_users_temp_table =f\"\"\"\n",
    "        CREATE TEMPORARY TABLE users as\n",
    "        (SELECT DISTINCT(all_users.id) as user_id\n",
    "        FROM prd_sj.sj_users as all_users\n",
    "        WHERE id IN ({users_list})\n",
    "                )\n",
    "            \"\"\"\n",
    "        return self.__query_redshift(query_users_temp_table)\n",
    "        \n",
    "        \n",
    "    def read_redshift_date(self, start_date, end_date, signup_app_id=[1,2,3], \n",
    "                           country=['US', 'AU', 'CA'], \n",
    "                           device=['Desktop', 'Smartphone', 'Tablet', 'Other Non-Mobile', 'Other Mobile',\n",
    "                                   'Robot', 'Smart-TV', 'Feature Phone'],\n",
    "                           min_points=500):\n",
    "                 \n",
    "        \"\"\"\n",
    "        read data from redshift based on users who signed up between specified start date and end date\n",
    "        start_date: string in date format (year/month/day)\n",
    "        end_date: string in date format (year/month/day)\n",
    "        signup_app_id: list or one signup id\n",
    "        country :  list or one country\n",
    "        device  :  list or one device\n",
    "        \"\"\"\n",
    "        print('reading...')\n",
    "        country, signup_app_id, device = self.__verify_arguments(country, signup_app_id, device)\n",
    "        \n",
    "        query_users_temp_table =f\"\"\"\n",
    "        CREATE TEMPORARY TABLE users as\n",
    "        (SELECT DISTINCT(all_users.id) as user_id\n",
    "        FROM prd_sj.sj_users as all_users\n",
    "        WHERE all_users.created_at::date >= '{start_date}'\n",
    "            AND all_users.created_at::date <= '{end_date}'\n",
    "            AND all_users.lifetime_points >={min_points}\n",
    "            AND all_users.signup_app_id IN ({signup_app_id})\n",
    "            AND all_users.id IN\n",
    "                    (SELECT user_id \n",
    "                    from prd_sj.sj_answers \n",
    "                    WHERE text_value IN ({country})\n",
    "                    AND question_id=8)\n",
    "            AND all_users.id IN\n",
    "                    (SELECT user_id \n",
    "                    from prd_sj.sj_answers \n",
    "                    WHERE text_value IN ({device})\n",
    "                    AND question_id=220)\n",
    "                )\n",
    "            \"\"\"\n",
    "        return self.__query_redshift(query_users_temp_table)\n",
    "    \n",
    "    \n",
    "    def __query_redshift(self, query_users_temp_table):\n",
    "        query_df_behavior = f\"\"\"\n",
    "        SELECT user_id, event_type_id, click_type_id, created_at, provider_id, device_platform, conversion_status_id, financial_points_promised, financial_points_paid,\n",
    "        financial_revenue_earned, device_browser, device_display_height, device_display_width, device_ip, device_ip_asn, activity_type_id, activity_conv_rate_all, impression_row, impression_column, impression_position \n",
    "        FROM prd_sj.user_activity_behavior\n",
    "        WHERE event_type_id IN (1,2)\n",
    "        AND user_id IN\n",
    "            (SELECT user_id\n",
    "                FROM users)\n",
    "        \"\"\"\n",
    "\n",
    "        query_df_login =\"\"\"\n",
    "        SELECT *\n",
    "        FROM prd_sj.user_login_activity\n",
    "        WHERE user_id IN\n",
    "            (SELECT user_id\n",
    "                FROM users)\n",
    "        \"\"\"\n",
    "\n",
    "        query_df_stat_hist =\"\"\"\n",
    "        SELECT *\n",
    "        FROM prd_sj.sj_user_status_history\n",
    "        WHERE user_id IN\n",
    "            (SELECT user_id\n",
    "                FROM users)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        query_df_prof_hist = \"\"\"\n",
    "        SELECT *\n",
    "        FROM prd_sj.sj_user_profile_history\n",
    "        WHERE user_id IN\n",
    "            (SELECT user_id\n",
    "                FROM users)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        query_df_completed_act = \"\"\"\n",
    "        SELECT *\n",
    "        FROM prd_sj.sj_user_completed_activities\n",
    "        WHERE user_id IN\n",
    "            (SELECT user_id\n",
    "                FROM users)\n",
    "        \"\"\"\n",
    "\n",
    "        query_df_answers = \"\"\"\n",
    "        SELECT *\n",
    "        FROM prd_sj.sj_answers\n",
    "        WHERE user_id IN\n",
    "            (SELECT user_id\n",
    "                FROM users)\n",
    "        \"\"\"\n",
    "\n",
    "        query_df_users = \"\"\"\n",
    "        SELECT *\n",
    "        FROM prd_sj.sj_users\n",
    "        WHERE id IN\n",
    "            (SELECT user_id\n",
    "                FROM users)\n",
    "        \"\"\"\n",
    "\n",
    "        print('Connecting')\n",
    "        start_time = time.time()\n",
    "        pr.connect_to_redshift(dbname   = self.__creds['db.redshift.db_name'], \n",
    "                               host     = self.__creds['db.redshift.host'], \n",
    "                               port     = self.__creds['db.redshift.port'], \n",
    "                               user     = self.__creds['db.redshift.user'], \n",
    "                               password = self.__creds['db.redshift.pass'])  \n",
    "\n",
    "        print('Connection established')\n",
    "        pr.exec_commit(query_users_temp_table)\n",
    "\n",
    "        df       = pr.redshift_to_pandas(query_df_behavior)\n",
    "        df_users = pr.redshift_to_pandas(query_df_users)\n",
    "        df_answers   =  pr.redshift_to_pandas(query_df_answers)\n",
    "        df_prof_hist =  pr.redshift_to_pandas(query_df_prof_hist)\n",
    "        df_stat_hist =  pr.redshift_to_pandas(query_df_stat_hist)\n",
    "        df_login     =  pr.redshift_to_pandas(query_df_login)\n",
    "        df_completed_act =  pr.redshift_to_pandas(query_df_completed_act)\n",
    "        pr.close_up_shop()\n",
    "\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print('done with time:', str(timedelta(seconds=time_elapsed)))\n",
    "        return df_users, df_completed_act, df_answers, df_prof_hist, df_stat_hist, df_login, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per month -> join\n",
    "# per class columns -> join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_directory(save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "        \n",
    "def write_to_s3(file_path, delete_local=True, bucket= 'disqo-data-science-dev'):\n",
    "    key = f\"fraud_detection{file_path[file_path.find('/'):]}\"\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_file(file_path)\n",
    "    print('Done writing to {}'.format('s3://{}/{}'.format(bucket, key)))\n",
    "    \n",
    "    if delete_local:\n",
    "        os.remove(file_path)\n",
    "        \n",
    "        \n",
    "def write_to_redshift_from_s3(file_path, redshift_table_name, append = False, bucket = \"disqo-data-science-dev\",\n",
    "                             credentials_file = './config/redshift.json'):\n",
    "    \"\"\"\n",
    "    Copy the data from s3 to redhshift, either create a new table or append to an existing one\n",
    "    maximum number of columns in redshift: 1600\n",
    "    \"\"\"\n",
    "   \n",
    "    with open(credentials_file) as raw_file:\n",
    "        creds = json.loads(raw_file.read())\n",
    "            \n",
    "    #CONNECT TO S3 AND TO REDSHIFT\n",
    "    pr.connect_to_s3(aws_access_key_id      = creds[\"aws_access_key_id_grp\"],\n",
    "                     aws_secret_access_key  = creds[\"aws_secret_access_key_grp\"],\n",
    "                     bucket                 = bucket,\n",
    "                     subdirectory           = None)\n",
    "    \n",
    "    pr.connect_to_redshift(dbname   = creds['db.redshift.db_name'], \n",
    "                       host     = creds['db.redshift.host'], \n",
    "                       port     = creds['db.redshift.port'], \n",
    "                       user     = creds['db.redshift.user'], \n",
    "                       password = creds['db.redshift.pass'])  \n",
    "    \n",
    "    #READ THE DATA FROM S3\n",
    "    data = pd.read_csv(f\"s3://{bucket}/{file_path}\")\n",
    "    \n",
    "    #VALIDATE COLUMN NAMES FOR REDSHIFT\n",
    "    data_frame = pr.validate_column_names(data)\n",
    "    \n",
    "    # CREATE AN EMPTY TABLE IN REDSHIFT\n",
    "    if not append:\n",
    "        pr.create_redshift_table(data_frame, redshift_table_name)\n",
    "\n",
    "    # # CREATE THE COPY STATEMENT TO SEND FROM S3 TO THE TABLE IN REDSHIFT\n",
    "    pr.s3_to_redshift(redshift_table_name, file_path)\n",
    "    pr.close_up_shop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"fraud_detection/data/processed/train/data_2020-06-01_from_2020-02-01_to_2020-03-01_.csv\"\n",
    "# redshift_table_name = 'dev_gasia.test_direct'\n",
    "# write_to_redshift_from_s3(file_path, redshift_table_name, append = False, bucket = \"disqo-data-science-dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_1 - > data_auto_2020-06-01_from_2020-02-01_to_2020-03-01_   \n",
    "# model_2 - > data_2020-07-01_from_2020-03-01_to_2020-04-01_\n",
    "# model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session\n",
    "# 1100\n",
    "# 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_fraud\n",
    "# fraud_input   model_1 febr->april.  model_2 febr->april.\n",
    "# fraud_output\n",
    "# redeem_input\n",
    "\n",
    "# email: data cut\n",
    "# append performance\n",
    "\n",
    "# cut_prob, without_cut_prob, autoencoder, convolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
